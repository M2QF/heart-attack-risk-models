{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Students\n",
    "- Ghaith Sarahnour\n",
    "- Roiseux Thomas\n",
    "\n",
    "# Introduction\n",
    "## Goal\n",
    "The goal of this project is to implement several machine learning algorithms to predict any risk of heart disease.\n",
    "## Dataset\n",
    "### Dataset description\n",
    "We will use [this dataset](http://www-stat.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data) to train our models. It contains 462 observations and 10 variables. The variables are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from typing import Tuple\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    BaggingClassifier,\n",
    "    AdaBoostClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    StackingClassifier,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "with open(r\"SAheartinfo.txt\") as f:\n",
    "    print(f.read())\n",
    "\n",
    "NUMBER_TRIALS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to visualize the dataset and its first lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"SAheart.txt\", sep=\",\", header=0, index_col=0)\n",
    "df.index.name = \"ID\"\n",
    "\n",
    "print(\"Head of dataframe\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"Description of dataframe\")\n",
    "display(df.describe())\n",
    "\n",
    "print(\"Info on dataframe\")\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to visualize the number of heart attacks in our dataset, and its proportion among the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Number of heart-attack cases among the total cases: {df['chd'].sum()} / {df['chd'].count()} ({df['chd'].sum() / 462 * 100}%).\"\n",
    ")\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Distribution of heart-attack cases\")\n",
    "\n",
    "plt.barh([\"Heart attack\", \"No heart attack\"], [df[\"chd\"].sum(), 462 - df[\"chd\"].sum()])\n",
    "plt.xlabel(\"Number of cases\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset analysis\n",
    "We now want to proceed to a a more in-depth analysis of the dataset. We will first look at the distribution of the variables and then at the correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"famhist\"] = df[\"famhist\"].apply(lambda x: 1 if x == \"Present\" else 0)\n",
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap=\"coolwarm\")\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(corr, annot=True, cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this preliminary analysis, we notice that we have a correlation between obesity and adiposity, as both of them are related to the amount of fat in a human body.\n",
    "We might then consider dropping one of them, as they are highly correlated. Let's drop `adiposity` for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"adiposity\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification models\n",
    "We are going to use three classification models:\n",
    "- Classification Decision Tree.\n",
    "- Bagging.\n",
    "- Random Forest.\n",
    "\n",
    "All these models will be trained on the dataset we just cleaned. We will then compare their performances.\n",
    "They will be provided by the `scikit-learn` library.\n",
    "\n",
    "For each model, we will try several hyperparameters and choose the best one.\n",
    "Then we will compare the performances of the three models, using the best parameters we found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Decision Tree\n",
    "### Model\n",
    "A decision tree is a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. It is a non-parametric supervised learning method used for classification and regression.\n",
    "### Hyperparameters\n",
    "#### Split\n",
    "The `split` hyperparameter is the strategy used to choose the split at each node. It can be `best` or `random`. The `best` strategy chooses the best split, while the `random` strategy chooses the best random split.\n",
    "#### Leaf\n",
    "The `leaf` hyperparameter is the minimum number of samples required to be at a leaf node. This hyperparameter is used to avoid overfitting.\n",
    "#### Deviance\n",
    "The `deviance` hyperparameter is the loss function used in the model. It is used to measure the quality of a split. It is the difference between the impurity of the parent node and the sum of the impurities of the child nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic model\n",
    "We are first going to build a tree classifier with default parameters, to have a baseline to compare our other models with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_decision_tree(\n",
    "    **kwargs,\n",
    ") -> Tuple[DecisionTreeClassifier, dict[str, float]]:\n",
    "    tree = DecisionTreeClassifier(**kwargs)\n",
    "    acc, prec, f1, roc = [], [], [], []\n",
    "    for _ in range(NUMBER_TRIALS):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            df.drop(columns=[\"chd\"]), df[\"chd\"], test_size=0.2, random_state=42\n",
    "        )\n",
    "        tree.fit(X_train, Y_train)\n",
    "        Y_pred = tree.predict(X_test)\n",
    "        acc.append(accuracy_score(Y_test, Y_pred))\n",
    "        prec.append(precision_score(Y_test, Y_pred))\n",
    "        f1.append(f1_score(Y_test, Y_pred))\n",
    "        roc.append(roc_auc_score(Y_test, Y_pred))\n",
    "    return tree, {\n",
    "        \"accuracy\": np.mean(acc),\n",
    "        \"precision\": np.mean(prec),\n",
    "        \"f1\": np.mean(f1),\n",
    "        \"roc_auc\": np.mean(roc),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree, power = build_and_run_decision_tree()\n",
    "\n",
    "power_df = pd.DataFrame(power, index=[\"Basic tree\"])\n",
    "power_df.index.name = \"Model\"\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This basic first tree classifier has a very bad score. Let's try to improve it.\n",
    "### Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree2, power = build_and_run_decision_tree(min_samples_split=10, min_samples_leaf=6)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [power_df, pd.DataFrame(power, index=[\"Tree with 10 samples, 6 leafs\"])]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, X_test, _, Y_test = train_test_split(\n",
    "    df.drop(columns=[\"chd\"]), df[\"chd\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"ROC curve for the trees\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, tree.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Default\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, tree2.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Tuned\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is more powerful than the previous one.\n",
    "Empirical testing shows that the best parameters are `min_samples_split=10` and `min_samples_leaf=6`.\n",
    "The ROC curve confirms that this model is better than the previous one.\n",
    "\n",
    "We are now going to try another classification model, the bagging classifier.\n",
    "\n",
    "## Bagging classifier\n",
    "We are going to try the same approach as before, with the bagging classifier.\n",
    "### Model\n",
    "A bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions to form a final prediction.\n",
    "### Hyperparameters\n",
    "#### Base estimator\n",
    "The `base_estimator` hyperparameter is the base estimator to fit on random subsets of the dataset. It can be `None` or a classifier. If `None`, then the base estimator is a decision tree.\n",
    "#### `n_estimators`\n",
    "The `n_estimators` hyperparameter is the number of base estimators in the ensemble.\n",
    "#### `max_samples`\n",
    "The `max_samples` hyperparameter is the number of samples to draw from `X` to train each base estimator.\n",
    "#### `max_features`\n",
    "The `max_features` hyperparameter is the number of features to draw from `X` to train each base estimator.\n",
    "#### `bootstrap`\n",
    "The `bootstrap` hyperparameter is whether samples are drawn with replacement. If `False`, sampling without replacement is performed.\n",
    "\n",
    "### Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_bagging(\n",
    "    **kwargs,\n",
    ") -> Tuple[BaggingClassifier, dict[str, float]]:\n",
    "    tree = BaggingClassifier(**kwargs)\n",
    "    acc, prec, f1, roc = [], [], [], []\n",
    "    for _ in range(NUMBER_TRIALS):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            df.drop(columns=[\"chd\"]), df[\"chd\"], test_size=0.2, random_state=42\n",
    "        )\n",
    "        tree.fit(X_train, Y_train)\n",
    "        Y_pred = tree.predict(X_test)\n",
    "        acc.append(accuracy_score(Y_test, Y_pred))\n",
    "        prec.append(precision_score(Y_test, Y_pred))\n",
    "        f1.append(f1_score(Y_test, Y_pred))\n",
    "        roc.append(roc_auc_score(Y_test, Y_pred))\n",
    "    return tree, {\n",
    "        \"accuracy\": np.mean(acc),\n",
    "        \"precision\": np.mean(prec),\n",
    "        \"f1\": np.mean(f1),\n",
    "        \"roc_auc\": np.mean(roc),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging, power = build_and_run_bagging()\n",
    "\n",
    "power_df = pd.concat([power_df, pd.DataFrame(power, index=[\"Basic Bagging\"])])\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is not very good. Let's try to improve it.\n",
    "\n",
    "### Hyperparameters tuning\n",
    "We will first try to use the previous good tree classifier as a base estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging2, power = build_and_run_bagging(\n",
    "    estimator=DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=6)\n",
    ")\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [power_df, pd.DataFrame(power, index=[\"Bagging with efficient tree\"])]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is better than the previous one. Let's try to improve it.\n",
    "It is nearly as powerful as the previous tree classifier.\n",
    "Let's try to tune the other hyperparameters to still improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging3, power = build_and_run_bagging(\n",
    "    estimator=DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=6),\n",
    "    max_samples=75,\n",
    ")\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(power, index=[\"Bagging with efficient tree, 75 max samples\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fixing `max_samples` to $75$ seems to be a good idea, as accuracy and precision seems to have lower variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"ROC curve for the Bagging classifiers\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, bagging.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Default\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, bagging2.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Efficient classifier\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, bagging3.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Tuned and efficient classifier\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the hyperparameters, we get a model that is better than the previous one.\n",
    "Let's now try the last one, the random forest classifier.\n",
    "\n",
    "## Random forest classifier\n",
    "### Model\n",
    "A random forest classifier is an ensemble meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n",
    "### Hyperparameters\n",
    "#### `n_estimators`\n",
    "The `n_estimators` hyperparameter is the number of trees in the forest.\n",
    "#### `max_depth`\n",
    "The `max_depth` hyperparameter is the maximum depth of the tree.\n",
    "#### `min_samples_split`\n",
    "The `min_samples_split` hyperparameter is the minimum number of samples required to split an internal node.\n",
    "#### `min_samples_leaf`\n",
    "The `min_samples_leaf` hyperparameter is the minimum number of samples required to be at a leaf node.\n",
    "### Basic model\n",
    "Let's first fit a random forest classifier with default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_rf(\n",
    "    **kwargs,\n",
    ") -> Tuple[RandomForestClassifier, dict[str, float]]:\n",
    "    tree = RandomForestClassifier(**kwargs)\n",
    "    acc, prec, f1, roc = [], [], [], []\n",
    "    for _ in range(NUMBER_TRIALS):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            df.drop(columns=[\"chd\"]), df[\"chd\"], test_size=0.2, random_state=42\n",
    "        )\n",
    "        tree.fit(X_train, Y_train)\n",
    "        Y_pred = tree.predict(X_test)\n",
    "        acc.append(accuracy_score(Y_test, Y_pred))\n",
    "        prec.append(precision_score(Y_test, Y_pred))\n",
    "        f1.append(f1_score(Y_test, Y_pred))\n",
    "        roc.append(roc_auc_score(Y_test, Y_pred))\n",
    "    return tree, {\n",
    "        \"accuracy\": np.mean(acc),\n",
    "        \"precision\": np.mean(prec),\n",
    "        \"f1\": np.mean(f1),\n",
    "        \"roc_auc\": np.mean(roc),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf, power = build_and_run_rf()\n",
    "\n",
    "power_df = pd.concat([power_df, pd.DataFrame(power, index=[\"Basic Random Forest\"])])\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters tuning\n",
    "This model seems to be better than the previous ones.\n",
    "Let's use the tuned hyperparameters of the previous tree classifier to see if we can improve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2, power = build_and_run_rf(\n",
    "    min_samples_split=10, min_samples_leaf=6, n_estimators=100, max_samples=75\n",
    ")\n",
    "\n",
    "power_df = pd.concat([power_df, pd.DataFrame(power, index=[\"Tuned Random Forest\"])])\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These hyperparameters seems to be the best ones for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"ROC curve for the random forests\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, rf.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Default\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, rf2.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Efficient classifier\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost classifier\n",
    "### Model\n",
    "An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.\n",
    "\n",
    "### Hyperparameters\n",
    "#### `base_estimator`\n",
    "The `base_estimator` hyperparameter is the base estimator from which the boosted ensemble is built. It must be a classifier.\n",
    "By default, it is a decision tree.\n",
    "#### `n_estimators`\n",
    "The `n_estimators` hyperparameter is the maximum number of estimators at which boosting is terminated.\n",
    "By default, it is $50$.\n",
    "#### `learning_rate`\n",
    "The `learning_rate` hyperparameter shrinks the contribution of each classifier by `learning_rate`. There is a trade-off between `learning_rate` and `n_estimators`.\n",
    "By default, it is $1$.\n",
    "\n",
    "### Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_adaboost(\n",
    "    **kwargs,\n",
    ") -> Tuple[AdaBoostClassifier, dict[str, float]]:\n",
    "    tree = AdaBoostClassifier(**kwargs)\n",
    "    acc, prec, f1, roc = [], [], [], []\n",
    "    for _ in range(NUMBER_TRIALS):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            df.drop(columns=[\"chd\"]), df[\"chd\"], test_size=0.2, random_state=42\n",
    "        )\n",
    "        tree.fit(X_train, Y_train)\n",
    "        Y_pred = tree.predict(X_test)\n",
    "        acc.append(accuracy_score(Y_test, Y_pred))\n",
    "        prec.append(precision_score(Y_test, Y_pred))\n",
    "        f1.append(f1_score(Y_test, Y_pred))\n",
    "        roc.append(roc_auc_score(Y_test, Y_pred))\n",
    "    return tree, {\n",
    "        \"accuracy\": np.mean(acc),\n",
    "        \"precision\": np.mean(prec),\n",
    "        \"f1\": np.mean(f1),\n",
    "        \"roc_auc\": np.mean(roc),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_class, power = build_and_run_adaboost()\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [power_df, pd.DataFrame(power, index=[\"Basic Adaboost classifier\"])]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic Adaboost classifier is already better than the tuend random forest but it doesn't overcome the tuned decision tree.\n",
    "Let's adjust its hyperparameters to see if we can improve it.\n",
    "\n",
    "### Hyperparameters tuning\n",
    "Let's try to tune the hyperparameters to see if we can improve the model.\n",
    "First, we can use the tuned hyperparameters of the previous decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_class, power = build_and_run_adaboost(\n",
    "    estimator=DecisionTreeClassifier(min_samples_split=10, min_samples_leaf=6)\n",
    ")\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(power, index=[\"Adaboost classifier with Tuned Decision Tree\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be worse than before.\n",
    "Let's try to adjust the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_class2, power = build_and_run_adaboost(learning_rate=0.5)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(power, index=[\"Adaboost classifier with learning rate 0.5\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_class3, power = build_and_run_adaboost(learning_rate=2)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [power_df, pd.DataFrame(power, index=[\"Adaboost classifier with learning rate 2\"])]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previous results shows that a lower learning rate seems to provide better results. Let's try to lower it even more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_class4, power = build_and_run_adaboost(learning_rate=0.25)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(power, index=[\"Adaboost classifier with learning rate 0.25\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ada_class5, power = build_and_run_adaboost(learning_rate=0.1)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(power, index=[\"Adaboost classifier with learning rate 0.1\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the chosen criteria, the best for `AdaBoostClassifier` is either `learning_rate=0.1` or `learning_rate=0.25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"ROC curve for the AdaBoost classifier\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, ada_class.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Default\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, ada_class4.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"AdaBoost with learning rate 0.25\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, ada_class5.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"AdaBoost with learning rate 0.1\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve shows that the best model is the one with `learning_rate=0.1`.\n",
    "\n",
    "## Gradient Boosting classifier\n",
    "### Model\n",
    "A gradient boosting classifier is a meta-estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses boosting to improve the predictive accuracy and control over-fitting.\n",
    "\n",
    "### Hyperparameters\n",
    "#### `loss`\n",
    "The `loss` hyperparameter is the loss function to be optimized. It must be `deviance` or `exponential`.\n",
    "By default, it is `deviance`.\n",
    "#### `learning_rate`\n",
    "The `learning_rate` hyperparameter shrinks the contribution of each classifier by `learning_rate`. There is a trade-off between `learning_rate` and `n_estimators`.\n",
    "By default, it is $0.1$.\n",
    "#### `n_estimators`\n",
    "The `n_estimators` hyperparameter is the number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n",
    "By default, it is $100$.\n",
    "#### `subsample`\n",
    "The `subsample` hyperparameter is the fraction of samples to be used for fitting the individual base learners. If smaller than $1.0$, this results in Stochastic Gradient Boosting. `subsample` interacts with the `n_estimators` hyperparameter. Choosing `subsample < 1.0` leads to a reduction of variance and an increase in bias.\n",
    "By default, it is $1.0$.\n",
    "#### `criterion`\n",
    "The `criterion` hyperparameter is the function to measure the quality of a split. It must be `friedman_mse`, `mse` or `mae`.\n",
    "By default, it is `friedman_mse`.\n",
    "\n",
    "### Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_gradient_boosting(\n",
    "    **kwargs,\n",
    ") -> Tuple[GradientBoostingClassifier, dict[str, float]]:\n",
    "    tree = GradientBoostingClassifier(**kwargs)\n",
    "    acc, prec, f1, roc = [], [], [], []\n",
    "    for _ in range(NUMBER_TRIALS):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            df.drop(columns=[\"chd\"]), df[\"chd\"], test_size=0.2, random_state=42\n",
    "        )\n",
    "        tree.fit(X_train, Y_train)\n",
    "        Y_pred = tree.predict(X_test)\n",
    "        acc.append(accuracy_score(Y_test, Y_pred))\n",
    "        prec.append(precision_score(Y_test, Y_pred))\n",
    "        f1.append(f1_score(Y_test, Y_pred))\n",
    "        roc.append(roc_auc_score(Y_test, Y_pred))\n",
    "    return tree, {\n",
    "        \"accuracy\": np.mean(acc),\n",
    "        \"precision\": np.mean(prec),\n",
    "        \"f1\": np.mean(f1),\n",
    "        \"roc_auc\": np.mean(roc),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb, power = build_and_run_gradient_boosting()\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [power_df, pd.DataFrame(power, index=[\"Basic Gradient Boosting classifier\"])]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model looks basically correct but we can improve it by tuning the hyperparameters.\n",
    "### Hyperparameters tuning\n",
    "Let's try to tune the hyperparameters to see if we can improve the model.\n",
    "Let's try first $1\\,000$ estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb2, power = build_and_run_gradient_boosting(n_estimators=150)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(power, index=[\"Gradient Boosting classifier with 150 estimators\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that increasing this number leads to worse results.\n",
    "Let's try 50 estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb3, power = build_and_run_gradient_boosting(n_estimators=50)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(power, index=[\"Gradient Boosting classifier with 50 estimators\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the same situation. Let's change the parameter and tune `learning_rate` to $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb4, power = build_and_run_gradient_boosting(learning_rate=0.01)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(\n",
    "            power, index=[\"Gradient Boosting classifier with learning rate 0.01\"]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reduces most of the scores. Let's try to increase it to $0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb5, power = build_and_run_gradient_boosting(learning_rate=0.2)\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(\n",
    "            power, index=[\"Gradient Boosting classifier with learning rate 0.2\"]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also reduces the scores.\n",
    "It seems that the basic model is the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"ROC curve for the Gradient Boosting classifier\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, gb.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Default\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, gb3.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Gradient Boosting with 50 estimators\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, gb4.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Gradient Boosting with learning rate 0.01\")\n",
    "fpr, tpr, _ = roc_curve(Y_test, gb5.predict_proba(X_test)[:, 1])\n",
    "plt.plot(fpr, tpr, label=\"Gradient Boosting with learning rate 0.2\")\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve shows that using `learning_rate=0.2` is not a good idea.\n",
    "However, other hyperparameters seems to give quite te same results, altough using `learning_rate=0.01` seems to be a good idea.\n",
    "\n",
    "## Stacking classifier\n",
    "### Model\n",
    "A stacking classifier is an ensemble-learning meta-classifier for stacking. It is useful for combining multiple estimators, for example, by averaging their predictions.\n",
    "### Hyperparameters\n",
    "#### `estimators`\n",
    "The `estimators` hyperparameter is a list of estimators to be fitted on the data. Each estimator must have a `fit` method. The final estimator is fitted on the concatenated results of the predictions of the estimators in `estimators`.\n",
    "#### `final_estimator`\n",
    "The `final_estimator` hyperparameter is an estimator which is used to combine the base estimators. It must have a `fit` method. If `None`, then a `LogisticRegression` classifier is used.\n",
    "#### `cv`\n",
    "The `cv` hyperparameter determines the cross-validation splitting strategy. It must be an iterable yielding pairs of train, test splits. If `None`, then `KFold` is used.\n",
    "#### `stack_method`\n",
    "The `stack_method` hyperparameter is the method used to stack the base estimators. It can be `auto`, `predict_proba` or `decision_function`.\n",
    "#### `n_jobs`\n",
    "The `n_jobs` hyperparameter is the number of jobs to run in parallel. It must be `None` or an integer. If `None`, then `1` is used.\n",
    "Here, we won't use it as we are exporing the model, not trying to optimize it for each CPU.\n",
    "\n",
    "### Basic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_run_stacking(\n",
    "    **kwargs,\n",
    ") -> Tuple[StackingClassifier, dict[str, float]]:\n",
    "    tree = StackingClassifier(**kwargs)\n",
    "    acc, prec, f1, roc = [], [], [], []\n",
    "    for _ in range(NUMBER_TRIALS):\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "            df.drop(columns=[\"chd\"]), df[\"chd\"], test_size=0.2, random_state=42\n",
    "        )\n",
    "        tree.fit(X_train, Y_train)\n",
    "        Y_pred = tree.predict(X_test)\n",
    "        acc.append(accuracy_score(Y_test, Y_pred))\n",
    "        prec.append(precision_score(Y_test, Y_pred))\n",
    "        f1.append(f1_score(Y_test, Y_pred))\n",
    "        roc.append(roc_auc_score(Y_test, Y_pred))\n",
    "    return tree, {\n",
    "        \"accuracy\": np.mean(acc),\n",
    "        \"precision\": np.mean(prec),\n",
    "        \"f1\": np.mean(f1),\n",
    "        \"roc_auc\": np.mean(roc),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking, power = build_and_run_stacking(estimators=[(\"rf\", rf), (\"gb\", gb)])\n",
    "\n",
    "power_df = pd.concat(\n",
    "    [\n",
    "        power_df,\n",
    "        pd.DataFrame(\n",
    "            power,\n",
    "            index=[\"Stacking classifier with random forest and gradient boosting\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(power_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of all classifiers\n",
    "Using the `power` data frame, we can now compare the three models we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_df.index.name = \"Model\"\n",
    "power_df.columns = [\"Accuracy\", \"Precision\", \"F1 score\", \"ROC AUC\"]\n",
    "\n",
    "\n",
    "power_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the best model is the AdaBoost classifier, using `learning_rate=0.1`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
